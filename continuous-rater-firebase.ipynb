{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import firebase_admin\n",
    "from firebase_admin import credentials\n",
    "from firebase_admin import firestore\n",
    "from firebase_admin import auth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add user input\n",
    "Fill in the variables in the cell below with the following information\n",
    "\n",
    "- `experiment` = name of experiment (outermost collection) in Firebase\n",
    "- `local_folder` = name of folder you want created locally and populated with data\n",
    "- `credential_path` = path to .json Firebase SDK service account key (explained [here](https://firebase.google.com/docs/admin/setup))\n",
    "- `firebase_project_name` = name of project in Firebase (click Project Overview in the Firebase console to see name)\n",
    "- `groups` = array of group names (as set by userGroup in src/utils.js and found as collections in the 'subject' document in your Firebase), each enclosed in single quotes and separated by commas\n",
    "- `rating_types` = array of rating types, each enclosed in single quotes and separated by commas (same as ratingTypes from src/utils.js)\n",
    "- `movie_names` = array of movie names, as written in Firebase stimuli table (no spaces), each enclosed in single quotes and separated by commas\n",
    "- `vid_lens` = dictionary of video durations, where each entry contains the movie name as key and the movie duration (in seconds) as value (e.g. {'movie1': 100, 'movie2': 150, 'movie3': 120}) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USER VARIABLES (FILL IN HERE)\n",
    "experiment = 'wasabi-online'\n",
    "local_folder = '/Users/jasondavis/Documents/MATLAB/state_affect_mapping_jad/wasabi_online/'\n",
    "credential_path = '/Users/jasondavis/Documents/Downloads/continuous-rater-jad-firebase-adminsdk-2ie9g-0080d92b8a.json'\n",
    "firebase_project_name = 'continuous-rater-jad'\n",
    "groups = ['Prolific Group'] # possible to only have on group, or can use multiple for organizational purposes\n",
    "rating_types = ['pleasant', 'unpleasant', 'calm', 'aroused', 'funny', 'happy', 'angry', 'sad', 'disgusted', 'afraid', 'suprised']\n",
    "movie_names = ['KungFuryPart1', 'KungFuryPart2']\n",
    "vid_lens = {'KungFuryPart1': 931, 'KungFuryPart2': 931} # example values, fill in with your own"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up communication with Firebase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cred = credentials.Certificate(credential_path)\n",
    "firebase_admin.initialize_app(cred, {\n",
    "  'projectId': firebase_project_name,\n",
    "})\n",
    "\n",
    "db = firestore.client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize data structures and directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize data structures\n",
    "subject_rating_dict = {}\n",
    "movie_q_count_dict = {}\n",
    "sub_to_mov = {}\n",
    "mov_to_sub = {}\n",
    "\n",
    "# initialize directory structure\n",
    "directories = []\n",
    "directories.append(f'{local_folder}/')\n",
    "directories.append(f'{local_folder}/Long/')\n",
    "directories.append(f'{local_folder}/Blanks/')\n",
    "directories.append(f'{local_folder}/Subjects/')\n",
    "directories.append(f'{local_folder}/Subjects/Incomplete/')\n",
    "directories.append(f'{local_folder}/Subjects/Groups/')\n",
    "directories.append(f'{local_folder}/Summary/')\n",
    "directories.append(f'{local_folder}/Ratings/')\n",
    "for movie in movie_names:\n",
    "    for rating in rating_types:\n",
    "        combo = movie + \"-\" + rating\n",
    "        movie_q_count_dict[combo] = 0\n",
    "        mov_to_sub[combo] = []\n",
    "        directories.append(f'{local_folder}/Ratings/{combo}/')\n",
    "        \n",
    "for directory in directories:\n",
    "    if not os.path.isdir(directory):\n",
    "        os.mkdir(directory) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create blank pandas dataframes of proper shape to fill in with rating data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use provided movie durations to create blank dataframe of proper length for each movie\n",
    "blanks_path = f'{local_folder}/Blanks/'\n",
    "for movie in movie_names:\n",
    "    curr_vid_len = vid_lens[movie] + 3 # add buffer time\n",
    "    init_ratings = np.full(curr_vid_len, -1)\n",
    "    curr_df = pd.DataFrame({'rating': init_ratings})\n",
    "    curr_df.to_csv(os.path.join(blanks_path, f'{movie}.csv')) # write out to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read blank dataframes back in and store in dictionary\n",
    "blanks_path = f'{local_folder}/Blanks/'\n",
    "blank_pd_dict = {}\n",
    "directory_list = glob.glob(os.path.join(blanks_path, '*.csv'))\n",
    "for file in directory_list:\n",
    "    curr_df = pd.read_csv(file)\n",
    "    drop_df = curr_df.drop(labels='Unnamed: 0', axis=1)\n",
    "    movie = os.path.basename(file).split('.')[0]\n",
    "    blank_pd_dict[movie] = drop_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine which subjects completed task, and save subject info to .csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1736348271.543045 2324159 config.cc:230] gRPC experiments enabled: call_status_override_on_cancellation, event_engine_dns, event_engine_listener, http2_stats_fix, monitoring_experiment, pick_first_new, trace_record_callops, work_serializer_clears_time_cache\n",
      "I0000 00:00:1736348271.569067 2324159 check_gcp_environment_no_op.cc:29] ALTS: Platforms other than Linux and Windows are not supported\n"
     ]
    }
   ],
   "source": [
    "# create good and bad subject lists (based on completion)\n",
    "good_id_master_list = []\n",
    "bad_id_master_list = []\n",
    "\n",
    "for group in groups:\n",
    "    good_id_list = [] # stores participants who completed HIT\n",
    "    bad_id_list = [] # stores participants who started but didn't complete HIT\n",
    "    sub_list = []\n",
    "\n",
    "    group_path = f'{experiment}/subjects/{group}'\n",
    "    group_collection = db.collection(group_path)\n",
    "    group_subs = group_collection.stream()\n",
    "\n",
    "    for sub in group_subs:\n",
    "        sub_dict = sub.to_dict()\n",
    "        if 'currentState' in sub_dict: # check to see if subject even started HIT\n",
    "            if sub_dict['currentState'] == 'debrief' or 'HIT_complete' in sub_dict: # only keep subs who finished\n",
    "                good_id_list.append(sub.id)\n",
    "                good_id_master_list.append(sub.id)\n",
    "                curr = pd.Series(sub_dict)\n",
    "                sub_list.append(curr)\n",
    "                file_path = f'{local_folder}/Subjects/{sub.id}.csv'\n",
    "                curr.to_csv(file_path)\n",
    "        else:\n",
    "            bad_id_list.append(sub.id)\n",
    "            bad_id_master_list.append(sub.id)\n",
    "            curr = pd.Series(sub_dict)\n",
    "            file_path = f'{local_folder}/Subjects/Incomplete/{sub.id}.csv'\n",
    "            curr.to_csv(file_path)\n",
    "\n",
    "    group_df = pd.DataFrame(sub_list)\n",
    "    group_df.to_csv(f'{local_folder}/Subjects/Groups/{group}.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get all ratings from subjects who completed task and store locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loops over subjects and gets ratings from database to store in local dictionary\n",
    "# also counts number of subjects that have rated each question\n",
    "# links subjects to movies and movies to subjects\n",
    "good_id_set = set(good_id_master_list) # removes repeats\n",
    "\n",
    "for good_id in good_id_set: \n",
    "    sub_to_mov[good_id] = []\n",
    "    movie_list = []\n",
    "    collection_path = f'{experiment}/ratings/{good_id}'\n",
    "    curr_sub_ratings = db.collection(collection_path)\n",
    "    HITs = curr_sub_ratings.stream()\n",
    "\n",
    "    curr_movie_dict = {}\n",
    "    for HIT in HITs:\n",
    "        sub_to_mov[good_id].append(HIT.id)\n",
    "        mov_to_sub[HIT.id].append(good_id)\n",
    "        curr_movie_dict[HIT.id] = HIT.to_dict()\n",
    "        movie_q_count_dict[HIT.id] += 1\n",
    "\n",
    "    if good_id in subject_rating_dict:\n",
    "        updated = subject_rating_dict[good_id].append(curr_movie_dict)\n",
    "        subject_rating_dict[good_id] = updated\n",
    "    else:\n",
    "        movie_list.append(curr_movie_dict)\n",
    "        subject_rating_dict[good_id] = movie_list\n",
    "\n",
    "movie_counts = pd.Series(movie_q_count_dict)\n",
    "movie_counts.to_csv(f'{local_folder}/Summary/Movie_Counts.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write out rating dictionary to .csv rating files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "253\n",
      "159\n",
      "2\n",
      "1\n",
      "355\n",
      "545\n",
      "2\n",
      "1\n",
      "464\n",
      "575\n",
      "2\n",
      "7\n",
      "2\n",
      "1\n",
      "89\n",
      "83\n",
      "248\n",
      "206\n",
      "2\n",
      "1\n",
      "2\n",
      "1\n",
      "19\n",
      "20\n",
      "325\n",
      "321\n",
      "2\n",
      "1\n",
      "2\n",
      "1\n",
      "89\n",
      "21\n",
      "61\n",
      "29\n",
      "12\n",
      "9\n",
      "14\n",
      "18\n",
      "14\n",
      "4\n",
      "52\n",
      "22\n",
      "141\n",
      "188\n",
      "38\n",
      "3\n",
      "47\n",
      "5\n",
      "777\n",
      "9\n",
      "56\n",
      "28\n",
      "41\n",
      "19\n",
      "156\n",
      "184\n",
      "206\n",
      "97\n",
      "155\n",
      "105\n",
      "206\n",
      "140\n",
      "10\n",
      "4\n",
      "73\n",
      "9\n",
      "164\n",
      "11\n",
      "197\n",
      "168\n",
      "2\n",
      "1\n",
      "43\n",
      "32\n",
      "415\n",
      "120\n",
      "149\n",
      "72\n",
      "2\n",
      "1\n",
      "2\n",
      "7\n",
      "25\n",
      "41\n",
      "2\n",
      "1\n",
      "299\n",
      "252\n",
      "15\n",
      "7\n",
      "2\n",
      "1\n",
      "190\n",
      "26\n",
      "869\n",
      "868\n",
      "2\n",
      "1\n",
      "97\n",
      "85\n",
      "270\n",
      "307\n",
      "2\n",
      "1\n",
      "155\n",
      "63\n",
      "146\n",
      "121\n",
      "2\n",
      "1\n",
      "265\n",
      "193\n",
      "2\n",
      "1\n",
      "63\n",
      "158\n",
      "22\n",
      "37\n",
      "2\n",
      "1\n",
      "130\n",
      "239\n",
      "2\n",
      "1\n",
      "135\n",
      "99\n",
      "28\n",
      "39\n",
      "146\n",
      "52\n",
      "2\n",
      "1\n",
      "2\n",
      "1\n",
      "9\n",
      "5\n",
      "2\n",
      "173\n",
      "25\n",
      "130\n",
      "182\n",
      "149\n",
      "531\n",
      "331\n",
      "3\n",
      "3\n",
      "2\n",
      "1\n",
      "2\n",
      "1\n",
      "19\n",
      "7\n",
      "41\n",
      "15\n",
      "11\n",
      "9\n",
      "43\n",
      "31\n",
      "75\n",
      "71\n",
      "143\n",
      "71\n",
      "441\n",
      "695\n",
      "496\n",
      "503\n",
      "2\n",
      "1\n",
      "27\n",
      "10\n",
      "169\n",
      "105\n",
      "2\n",
      "10\n",
      "8\n",
      "6\n",
      "2\n",
      "1\n",
      "2\n",
      "1\n",
      "2\n",
      "1\n",
      "40\n",
      "40\n",
      "2\n",
      "1\n",
      "2\n",
      "1\n",
      "2\n",
      "1\n",
      "187\n",
      "431\n",
      "96\n",
      "41\n",
      "28\n",
      "24\n",
      "113\n",
      "109\n",
      "2\n",
      "1\n",
      "28\n",
      "21\n",
      "2\n",
      "1\n",
      "27\n",
      "12\n",
      "2\n",
      "1\n",
      "5\n",
      "3\n",
      "31\n",
      "17\n",
      "2\n",
      "1\n",
      "273\n",
      "242\n",
      "11\n",
      "3\n",
      "2\n",
      "1\n",
      "2\n",
      "1\n",
      "618\n",
      "638\n",
      "2\n",
      "1\n",
      "243\n",
      "175\n",
      "25\n",
      "87\n",
      "15\n",
      "1\n",
      "2\n",
      "1\n",
      "2\n",
      "1\n",
      "17\n",
      "11\n",
      "6\n",
      "4\n",
      "2\n",
      "15\n",
      "50\n",
      "126\n",
      "2\n",
      "1\n",
      "25\n",
      "9\n",
      "440\n",
      "434\n",
      "28\n",
      "25\n",
      "2\n",
      "1\n",
      "2\n",
      "1\n",
      "215\n",
      "200\n",
      "2\n",
      "1\n",
      "4\n",
      "3\n",
      "7\n",
      "1\n",
      "2\n",
      "1\n",
      "2\n",
      "1\n",
      "407\n",
      "531\n",
      "2\n",
      "34\n",
      "60\n",
      "85\n",
      "2\n",
      "1\n",
      "164\n",
      "102\n",
      "2\n",
      "1\n",
      "11\n",
      "4\n",
      "295\n",
      "357\n",
      "36\n",
      "3\n",
      "102\n",
      "20\n",
      "36\n",
      "24\n",
      "298\n",
      "242\n",
      "25\n",
      "12\n",
      "2\n",
      "1\n",
      "80\n",
      "87\n",
      "13\n",
      "5\n",
      "160\n",
      "153\n",
      "53\n",
      "58\n",
      "356\n",
      "336\n",
      "113\n",
      "123\n",
      "2\n",
      "26\n",
      "61\n",
      "57\n",
      "2\n",
      "1\n",
      "2\n",
      "1\n",
      "2\n",
      "75\n",
      "2\n",
      "1\n",
      "16\n",
      "11\n",
      "2\n",
      "53\n",
      "11\n",
      "3\n",
      "11\n",
      "3\n",
      "3\n",
      "1\n",
      "108\n",
      "89\n",
      "147\n",
      "59\n",
      "2\n",
      "1\n",
      "2\n",
      "1\n",
      "2\n",
      "1\n",
      "13\n",
      "25\n",
      "77\n",
      "60\n",
      "127\n",
      "82\n",
      "2\n",
      "1\n",
      "2\n",
      "1\n",
      "2\n",
      "74\n"
     ]
    }
   ],
   "source": [
    "# writes file containing rating-timestamp pairs for each subject to folder for each movie-rating pairing\n",
    "for good_id in good_id_set:\n",
    "    curr_sub = subject_rating_dict[good_id]\n",
    "    for dictionary in curr_sub:\n",
    "        for movie_rating in dictionary:\n",
    "            words = movie_rating.split('-')\n",
    "            blank_mov_pd = blank_pd_dict[words[0]].copy()\n",
    "            rating_dict = dictionary[movie_rating]\n",
    "            count = 0\n",
    "            print(len(rating_dict))\n",
    "            for timestamp in rating_dict:\n",
    "                    if timestamp.isdigit():\n",
    "                        blank_mov_pd.iloc[int(timestamp)] = rating_dict[timestamp]\n",
    "                    else:\n",
    "                        continue\n",
    "            file_path = f'{local_folder}/Ratings/{movie_rating}/{good_id}.csv'  \n",
    "            blank_mov_pd.to_csv(file_path)      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine subject and rating info together to create long format file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the base frame for appending\n",
    "cols = ['workerId', 'movie', 'ratingType', 'HIT_complete', 'age', 'assignmentId', 'birth', 'consentStatus', 'currentState', \\\n",
    "'ethnicity', 'feedback', 'handed', 'hitId', 'nativeLang', 'race', 'sex', 'startTime', \\\n",
    "'userId', 'mostRecentTime', 'timeStamp', 'ratingScore']\n",
    "\n",
    "base_frame = pd.DataFrame(np.nan, index=[0], columns=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this take individual local csvs and turns them into long format per movie per rating per subject\n",
    "dir_list = glob.glob(f'{local_folder}/Ratings/*')\n",
    "master_long = base_frame.copy()\n",
    "\n",
    "for directory in dir_list:\n",
    "    path = directory + '/*.csv'\n",
    "    rating_list = glob.glob(path)\n",
    "    for file in rating_list:\n",
    "        sub_id = os.path.basename(file).split('.')[0]\n",
    "        movie_rating = file.split('/')[2]\n",
    "        movie = movie_rating.split('-')[0]\n",
    "        rating = movie_rating.split('-')[0]       \n",
    "\n",
    "        # this should check and not rewrite files that already exist (speeds up process)\n",
    "        if not os.path.isfile(f'{local_folder}/Long/{movie}-{rating}-{sub_id}.csv'):\n",
    "            subject_long = base_frame.copy()\n",
    "            rating_pd = pd.read_csv(file)\n",
    "            sub_path = f'{local_folder}/Subjects/{sub_id}.csv'\n",
    "            if os.path.isfile(sub_path):\n",
    "                sub_pd = pd.read_csv(sub_path)\n",
    "                title_list = sub_pd['Unnamed: 0'].values\n",
    "                rename_dict = {}\n",
    "                counter = 0\n",
    "                for title in title_list:\n",
    "                    rename_dict[counter] = title\n",
    "                    counter += 1   \n",
    "                new_sub_pd = sub_pd.transpose().rename(columns=rename_dict).drop(['Unnamed: 0'])\n",
    "\n",
    "                new_pd = base_frame.copy()\n",
    "                new_pd['movie'] = movie\n",
    "                new_pd['ratingType'] = rating\n",
    "\n",
    "                for category in base_frame:\n",
    "                    if category in new_sub_pd:\n",
    "                        new_pd[category] = new_sub_pd[category].values\n",
    "\n",
    "                timestamp_dict = rating_pd.transpose().drop(['Unnamed: 0'])\n",
    "                copy_pd = new_pd.copy()\n",
    "                prevScore = -1\n",
    "                rating_counter = 0\n",
    "                for timestamp in timestamp_dict:\n",
    "                    ratingScore = timestamp_dict[timestamp].values\n",
    "                    if ratingScore != -1:\n",
    "                        prevScore = ratingScore\n",
    "                    else:\n",
    "                        ratingScore = prevScore\n",
    "\n",
    "                    copy_pd['timeStamp'] = timestamp\n",
    "                    copy_pd['ratingScore'] = ratingScore\n",
    "                    subject_long = pd.concat([subject_long, copy_pd], ignore_index=True)\n",
    "\n",
    "                subject_long = subject_long.drop([0])\n",
    "                subject_long.to_csv(f'{local_folder}/Long/{movie}-{rating}-{sub_id}.csv')                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this appends all individual long format files into one giant long format panda\n",
    "master_long = base_frame.copy()\n",
    "long_list = glob.glob(f'{local_folder}/Long/*.csv')\n",
    "\n",
    "for file in long_list:\n",
    "    curr_pd = pd.read_csv(file)\n",
    "    master_long = pd.concat([master_long, curr_pd])\n",
    "\n",
    "master_long.drop([0])\n",
    "master_long.to_csv(f'{local_folder}/master_long.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in saved copy\n",
    "master_long = pd.read_csv(f'{local_folder}/master_long.csv')\n",
    "master_long.drop(labels=[0], inplace=True)\n",
    "master_long.drop(labels=['Unnamed: 0', 'Unnamed: 0.1'], axis=1, inplace=True)\n",
    "sub_list = master_long['workerId'].unique()\n",
    "movie_list = master_long['movie'].unique()\n",
    "rating_list = master_long['ratingType'].unique()\n",
    "movie = movie_list[0]\n",
    "rating = rating_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>workerId</th>\n",
       "      <th>movie</th>\n",
       "      <th>ratingType</th>\n",
       "      <th>HIT_complete</th>\n",
       "      <th>age</th>\n",
       "      <th>assignmentId</th>\n",
       "      <th>birth</th>\n",
       "      <th>consentStatus</th>\n",
       "      <th>currentState</th>\n",
       "      <th>ethnicity</th>\n",
       "      <th>...</th>\n",
       "      <th>handed</th>\n",
       "      <th>hitId</th>\n",
       "      <th>nativeLang</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>startTime</th>\n",
       "      <th>userId</th>\n",
       "      <th>mostRecentTime</th>\n",
       "      <th>timeStamp</th>\n",
       "      <th>ratingScore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>453829892.0</td>\n",
       "      <td>jasondavis</td>\n",
       "      <td>jasondavis</td>\n",
       "      <td>2024-11-24 23:06:47.564000+00:00</td>\n",
       "      <td>30.0</td>\n",
       "      <td>453829892.0</td>\n",
       "      <td>Manila, Metro Manila, Philippines</td>\n",
       "      <td>signed</td>\n",
       "      <td>complete</td>\n",
       "      <td>not_hispanic</td>\n",
       "      <td>...</td>\n",
       "      <td>right</td>\n",
       "      <td>453829892.0</td>\n",
       "      <td>Tagalog (Filipino)</td>\n",
       "      <td>['Asian / Asian-American']</td>\n",
       "      <td>female</td>\n",
       "      <td>2024-11-24 23:06:47.564000+00:00</td>\n",
       "      <td>MzBlQ7I3mfhoq87srrwl78MCuEU2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>453829892.0</td>\n",
       "      <td>jasondavis</td>\n",
       "      <td>jasondavis</td>\n",
       "      <td>2024-11-24 23:06:47.564000+00:00</td>\n",
       "      <td>30.0</td>\n",
       "      <td>453829892.0</td>\n",
       "      <td>Manila, Metro Manila, Philippines</td>\n",
       "      <td>signed</td>\n",
       "      <td>complete</td>\n",
       "      <td>not_hispanic</td>\n",
       "      <td>...</td>\n",
       "      <td>right</td>\n",
       "      <td>453829892.0</td>\n",
       "      <td>Tagalog (Filipino)</td>\n",
       "      <td>['Asian / Asian-American']</td>\n",
       "      <td>female</td>\n",
       "      <td>2024-11-24 23:06:47.564000+00:00</td>\n",
       "      <td>MzBlQ7I3mfhoq87srrwl78MCuEU2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>453829892.0</td>\n",
       "      <td>jasondavis</td>\n",
       "      <td>jasondavis</td>\n",
       "      <td>2024-11-24 23:06:47.564000+00:00</td>\n",
       "      <td>30.0</td>\n",
       "      <td>453829892.0</td>\n",
       "      <td>Manila, Metro Manila, Philippines</td>\n",
       "      <td>signed</td>\n",
       "      <td>complete</td>\n",
       "      <td>not_hispanic</td>\n",
       "      <td>...</td>\n",
       "      <td>right</td>\n",
       "      <td>453829892.0</td>\n",
       "      <td>Tagalog (Filipino)</td>\n",
       "      <td>['Asian / Asian-American']</td>\n",
       "      <td>female</td>\n",
       "      <td>2024-11-24 23:06:47.564000+00:00</td>\n",
       "      <td>MzBlQ7I3mfhoq87srrwl78MCuEU2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>453829892.0</td>\n",
       "      <td>jasondavis</td>\n",
       "      <td>jasondavis</td>\n",
       "      <td>2024-11-24 23:06:47.564000+00:00</td>\n",
       "      <td>30.0</td>\n",
       "      <td>453829892.0</td>\n",
       "      <td>Manila, Metro Manila, Philippines</td>\n",
       "      <td>signed</td>\n",
       "      <td>complete</td>\n",
       "      <td>not_hispanic</td>\n",
       "      <td>...</td>\n",
       "      <td>right</td>\n",
       "      <td>453829892.0</td>\n",
       "      <td>Tagalog (Filipino)</td>\n",
       "      <td>['Asian / Asian-American']</td>\n",
       "      <td>female</td>\n",
       "      <td>2024-11-24 23:06:47.564000+00:00</td>\n",
       "      <td>MzBlQ7I3mfhoq87srrwl78MCuEU2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>48.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>453829892.0</td>\n",
       "      <td>jasondavis</td>\n",
       "      <td>jasondavis</td>\n",
       "      <td>2024-11-24 23:06:47.564000+00:00</td>\n",
       "      <td>30.0</td>\n",
       "      <td>453829892.0</td>\n",
       "      <td>Manila, Metro Manila, Philippines</td>\n",
       "      <td>signed</td>\n",
       "      <td>complete</td>\n",
       "      <td>not_hispanic</td>\n",
       "      <td>...</td>\n",
       "      <td>right</td>\n",
       "      <td>453829892.0</td>\n",
       "      <td>Tagalog (Filipino)</td>\n",
       "      <td>['Asian / Asian-American']</td>\n",
       "      <td>female</td>\n",
       "      <td>2024-11-24 23:06:47.564000+00:00</td>\n",
       "      <td>MzBlQ7I3mfhoq87srrwl78MCuEU2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175588</th>\n",
       "      <td>593527369.0</td>\n",
       "      <td>jasondavis</td>\n",
       "      <td>jasondavis</td>\n",
       "      <td>2024-11-19 03:29:41.867000+00:00</td>\n",
       "      <td>38.0</td>\n",
       "      <td>593527369.0</td>\n",
       "      <td>Payson, Utah, USA</td>\n",
       "      <td>signed</td>\n",
       "      <td>complete</td>\n",
       "      <td>not_hispanic</td>\n",
       "      <td>...</td>\n",
       "      <td>right</td>\n",
       "      <td>593527369.0</td>\n",
       "      <td>English</td>\n",
       "      <td>['White / Caucasian']</td>\n",
       "      <td>female</td>\n",
       "      <td>2024-11-19 03:29:41.867000+00:00</td>\n",
       "      <td>z5mBRgqrEkda0UCfqIkXii3r3sw2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>929.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175589</th>\n",
       "      <td>593527369.0</td>\n",
       "      <td>jasondavis</td>\n",
       "      <td>jasondavis</td>\n",
       "      <td>2024-11-19 03:29:41.867000+00:00</td>\n",
       "      <td>38.0</td>\n",
       "      <td>593527369.0</td>\n",
       "      <td>Payson, Utah, USA</td>\n",
       "      <td>signed</td>\n",
       "      <td>complete</td>\n",
       "      <td>not_hispanic</td>\n",
       "      <td>...</td>\n",
       "      <td>right</td>\n",
       "      <td>593527369.0</td>\n",
       "      <td>English</td>\n",
       "      <td>['White / Caucasian']</td>\n",
       "      <td>female</td>\n",
       "      <td>2024-11-19 03:29:41.867000+00:00</td>\n",
       "      <td>z5mBRgqrEkda0UCfqIkXii3r3sw2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>930.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175590</th>\n",
       "      <td>593527369.0</td>\n",
       "      <td>jasondavis</td>\n",
       "      <td>jasondavis</td>\n",
       "      <td>2024-11-19 03:29:41.867000+00:00</td>\n",
       "      <td>38.0</td>\n",
       "      <td>593527369.0</td>\n",
       "      <td>Payson, Utah, USA</td>\n",
       "      <td>signed</td>\n",
       "      <td>complete</td>\n",
       "      <td>not_hispanic</td>\n",
       "      <td>...</td>\n",
       "      <td>right</td>\n",
       "      <td>593527369.0</td>\n",
       "      <td>English</td>\n",
       "      <td>['White / Caucasian']</td>\n",
       "      <td>female</td>\n",
       "      <td>2024-11-19 03:29:41.867000+00:00</td>\n",
       "      <td>z5mBRgqrEkda0UCfqIkXii3r3sw2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>931.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175591</th>\n",
       "      <td>593527369.0</td>\n",
       "      <td>jasondavis</td>\n",
       "      <td>jasondavis</td>\n",
       "      <td>2024-11-19 03:29:41.867000+00:00</td>\n",
       "      <td>38.0</td>\n",
       "      <td>593527369.0</td>\n",
       "      <td>Payson, Utah, USA</td>\n",
       "      <td>signed</td>\n",
       "      <td>complete</td>\n",
       "      <td>not_hispanic</td>\n",
       "      <td>...</td>\n",
       "      <td>right</td>\n",
       "      <td>593527369.0</td>\n",
       "      <td>English</td>\n",
       "      <td>['White / Caucasian']</td>\n",
       "      <td>female</td>\n",
       "      <td>2024-11-19 03:29:41.867000+00:00</td>\n",
       "      <td>z5mBRgqrEkda0UCfqIkXii3r3sw2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>932.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175592</th>\n",
       "      <td>593527369.0</td>\n",
       "      <td>jasondavis</td>\n",
       "      <td>jasondavis</td>\n",
       "      <td>2024-11-19 03:29:41.867000+00:00</td>\n",
       "      <td>38.0</td>\n",
       "      <td>593527369.0</td>\n",
       "      <td>Payson, Utah, USA</td>\n",
       "      <td>signed</td>\n",
       "      <td>complete</td>\n",
       "      <td>not_hispanic</td>\n",
       "      <td>...</td>\n",
       "      <td>right</td>\n",
       "      <td>593527369.0</td>\n",
       "      <td>English</td>\n",
       "      <td>['White / Caucasian']</td>\n",
       "      <td>female</td>\n",
       "      <td>2024-11-19 03:29:41.867000+00:00</td>\n",
       "      <td>z5mBRgqrEkda0UCfqIkXii3r3sw2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>933.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>175592 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           workerId       movie  ratingType                      HIT_complete  \\\n",
       "1       453829892.0  jasondavis  jasondavis  2024-11-24 23:06:47.564000+00:00   \n",
       "2       453829892.0  jasondavis  jasondavis  2024-11-24 23:06:47.564000+00:00   \n",
       "3       453829892.0  jasondavis  jasondavis  2024-11-24 23:06:47.564000+00:00   \n",
       "4       453829892.0  jasondavis  jasondavis  2024-11-24 23:06:47.564000+00:00   \n",
       "5       453829892.0  jasondavis  jasondavis  2024-11-24 23:06:47.564000+00:00   \n",
       "...             ...         ...         ...                               ...   \n",
       "175588  593527369.0  jasondavis  jasondavis  2024-11-19 03:29:41.867000+00:00   \n",
       "175589  593527369.0  jasondavis  jasondavis  2024-11-19 03:29:41.867000+00:00   \n",
       "175590  593527369.0  jasondavis  jasondavis  2024-11-19 03:29:41.867000+00:00   \n",
       "175591  593527369.0  jasondavis  jasondavis  2024-11-19 03:29:41.867000+00:00   \n",
       "175592  593527369.0  jasondavis  jasondavis  2024-11-19 03:29:41.867000+00:00   \n",
       "\n",
       "         age  assignmentId                              birth consentStatus  \\\n",
       "1       30.0   453829892.0  Manila, Metro Manila, Philippines        signed   \n",
       "2       30.0   453829892.0  Manila, Metro Manila, Philippines        signed   \n",
       "3       30.0   453829892.0  Manila, Metro Manila, Philippines        signed   \n",
       "4       30.0   453829892.0  Manila, Metro Manila, Philippines        signed   \n",
       "5       30.0   453829892.0  Manila, Metro Manila, Philippines        signed   \n",
       "...      ...           ...                                ...           ...   \n",
       "175588  38.0   593527369.0                  Payson, Utah, USA        signed   \n",
       "175589  38.0   593527369.0                  Payson, Utah, USA        signed   \n",
       "175590  38.0   593527369.0                  Payson, Utah, USA        signed   \n",
       "175591  38.0   593527369.0                  Payson, Utah, USA        signed   \n",
       "175592  38.0   593527369.0                  Payson, Utah, USA        signed   \n",
       "\n",
       "       currentState     ethnicity  ... handed        hitId  \\\n",
       "1          complete  not_hispanic  ...  right  453829892.0   \n",
       "2          complete  not_hispanic  ...  right  453829892.0   \n",
       "3          complete  not_hispanic  ...  right  453829892.0   \n",
       "4          complete  not_hispanic  ...  right  453829892.0   \n",
       "5          complete  not_hispanic  ...  right  453829892.0   \n",
       "...             ...           ...  ...    ...          ...   \n",
       "175588     complete  not_hispanic  ...  right  593527369.0   \n",
       "175589     complete  not_hispanic  ...  right  593527369.0   \n",
       "175590     complete  not_hispanic  ...  right  593527369.0   \n",
       "175591     complete  not_hispanic  ...  right  593527369.0   \n",
       "175592     complete  not_hispanic  ...  right  593527369.0   \n",
       "\n",
       "                nativeLang                        race     sex  \\\n",
       "1       Tagalog (Filipino)  ['Asian / Asian-American']  female   \n",
       "2       Tagalog (Filipino)  ['Asian / Asian-American']  female   \n",
       "3       Tagalog (Filipino)  ['Asian / Asian-American']  female   \n",
       "4       Tagalog (Filipino)  ['Asian / Asian-American']  female   \n",
       "5       Tagalog (Filipino)  ['Asian / Asian-American']  female   \n",
       "...                    ...                         ...     ...   \n",
       "175588             English       ['White / Caucasian']  female   \n",
       "175589             English       ['White / Caucasian']  female   \n",
       "175590             English       ['White / Caucasian']  female   \n",
       "175591             English       ['White / Caucasian']  female   \n",
       "175592             English       ['White / Caucasian']  female   \n",
       "\n",
       "                               startTime                        userId  \\\n",
       "1       2024-11-24 23:06:47.564000+00:00  MzBlQ7I3mfhoq87srrwl78MCuEU2   \n",
       "2       2024-11-24 23:06:47.564000+00:00  MzBlQ7I3mfhoq87srrwl78MCuEU2   \n",
       "3       2024-11-24 23:06:47.564000+00:00  MzBlQ7I3mfhoq87srrwl78MCuEU2   \n",
       "4       2024-11-24 23:06:47.564000+00:00  MzBlQ7I3mfhoq87srrwl78MCuEU2   \n",
       "5       2024-11-24 23:06:47.564000+00:00  MzBlQ7I3mfhoq87srrwl78MCuEU2   \n",
       "...                                  ...                           ...   \n",
       "175588  2024-11-19 03:29:41.867000+00:00  z5mBRgqrEkda0UCfqIkXii3r3sw2   \n",
       "175589  2024-11-19 03:29:41.867000+00:00  z5mBRgqrEkda0UCfqIkXii3r3sw2   \n",
       "175590  2024-11-19 03:29:41.867000+00:00  z5mBRgqrEkda0UCfqIkXii3r3sw2   \n",
       "175591  2024-11-19 03:29:41.867000+00:00  z5mBRgqrEkda0UCfqIkXii3r3sw2   \n",
       "175592  2024-11-19 03:29:41.867000+00:00  z5mBRgqrEkda0UCfqIkXii3r3sw2   \n",
       "\n",
       "       mostRecentTime  timeStamp  ratingScore  \n",
       "1                 NaN        0.0         50.0  \n",
       "2                 NaN        1.0         50.0  \n",
       "3                 NaN        2.0         50.0  \n",
       "4                 NaN        3.0         48.0  \n",
       "5                 NaN        4.0          0.0  \n",
       "...               ...        ...          ...  \n",
       "175588            NaN      929.0          0.0  \n",
       "175589            NaN      930.0          0.0  \n",
       "175590            NaN      931.0          0.0  \n",
       "175591            NaN      932.0          0.0  \n",
       "175592            NaN      933.0          0.0  \n",
       "\n",
       "[175592 rows x 21 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "master_long"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
